import streamlit as st
import pandas as pd
import requests
import feedparser
from newspaper import Article, Config
import jieba
import nltk
import time

# --- 1. NLTK è‡ªå‹•ä¿®å¾©å€ (è§£æ±º punkt_tab éŒ¯èª¤) ---
# Streamlit Cloud æ¯æ¬¡å•Ÿå‹•éƒ½æ˜¯å…¨æ–°ç’°å¢ƒï¼Œå¿…é ˆå¼·åˆ¶ä¸‹è¼‰å­—å…¸æª”
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="Google News AI æ‘˜è¦", page_icon="ğŸ“°", layout="wide")
st.title("ğŸ“° Google News AI æ‘˜è¦ (Thunderbit æ–¹æ³•ä¿®æ­£ç‰ˆ)")
st.markdown("ä¾†æºï¼š**Google News** | æŠ€è¡“ï¼š**URL è§£ç¢¼** + **LSA æ‘˜è¦**")

# --- 3. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def decode_google_news_url(source_url):
    """
    é—œéµå‡½å¼ï¼šè§£æ±º Google News é€£çµè½‰å€å•é¡Œã€‚
    Google çš„ RSS çµ¦çš„æ˜¯ 'news.google.com/...' çš„è·³è½‰é€£çµï¼Œ
    ç›´æ¥çˆ¬æœƒå¤±æ•—ã€‚å¿…é ˆé€é requests ç²å–æœ€çµ‚çš„çœŸå¯¦ç¶²å€ã€‚
    """
    try:
        # æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨çš„ Headersï¼Œé¨™é Google çš„é˜²çˆ¬èŸ²æ©Ÿåˆ¶
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Referer': 'https://news.google.com/'
        }
        
        # ç™¼é€è«‹æ±‚ï¼Œallow_redirects=True æœƒè‡ªå‹•è·Ÿéš¨è·³è½‰ç›´åˆ°æœ€å¾Œçš„çœŸå¯¦ç¶²å€
        response = requests.get(source_url, headers=headers, timeout=10, allow_redirects=True)
        
        # æª¢æŸ¥æ˜¯å¦çœŸçš„è·³è½‰åˆ°äº†å¤–éƒ¨ç¶²ç«™ (ç¶²å€ä¸åŒ…å« google.com)
        if 'google.com' not in response.url:
            return response.url
        
        # å¦‚æœé‚„æ˜¯åœ¨ google ç¶²åŸŸï¼Œå¯èƒ½æ˜¯è¢« Consent é é¢æ“‹ä½äº†ï¼Œå›å‚³åŸç¶²å€è©¦è©¦é‹æ°£
        return source_url
    except Exception as e:
        print(f"è§£ç¢¼å¤±æ•—: {e}")
        return source_url

def sumy_summarize(text, sentence_count=3):
    """ä½¿ç”¨ Sumy + Jieba é€²è¡Œä¸­æ–‡èƒå–å¼æ‘˜è¦"""
    try:
        if not text: return "ç„¡å…§å®¹"
        
        # ä¸­æ–‡æ–·è©
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        
        # åˆå§‹åŒ–æ‘˜è¦å™¨
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        
        result = ""
        for sentence in summary_sentences:
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
        return result
    except Exception as e:
        return f"æ‘˜è¦é‹ç®—éŒ¯èª¤: {e}"

def extract_and_process(google_url):
    """
    æµç¨‹ï¼šè§£ç¢¼ Google é€£çµ -> çˆ¬å–çœŸå¯¦ç¶²é  -> ç”¢ç”Ÿæ‘˜è¦
    """
    try:
        # æ­¥é©Ÿ 1: ç²å–çœŸå¯¦ç¶²å€ (é€™æ˜¯ä¹‹å‰å¤±æ•—çš„é—œéµ)
        real_url = decode_google_news_url(google_url)
        
        # æ­¥é©Ÿ 2: è¨­å®šçˆ¬èŸ² Config (å½è£æˆç€è¦½å™¨)
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
        config.request_timeout = 10
        
        article = Article(real_url, config=config)
        article.download()
        article.parse()
        
        # æª¢æŸ¥å…§å®¹é•·åº¦
        if len(article.text) < 50:
             # å¦‚æœæ­£æ–‡æŠ“ä¸åˆ°ï¼Œå˜—è©¦æŠ“ Meta Description ç•¶ä½œå‚™æ¡ˆ
             if article.meta_description and len(article.meta_description) > 10:
                 return f"ğŸ“Œ (ä¾†æºç°¡ä»‹) {article.meta_description}", real_url
             return "âš ï¸ ç„¡æ³•æŠ“å–å…§å®¹ (ç¶²ç«™é˜»æ“‹çˆ¬èŸ²)", real_url

        # æ­¥é©Ÿ 3: æ‘˜è¦
        summary = sumy_summarize(article.text, sentence_count=3)
        return summary, real_url
        
    except Exception as e:
        return f"âŒ è™•ç†éŒ¯èª¤: {str(e)}", google_url

def search_google_news_rss(keyword, limit=5):
    """
    ä½¿ç”¨ Google News RSS (æœ€æ¥è¿‘ Thunderbit æŒ‡å—çš„ Python å¯¦ä½œæ–¹å¼)
    """
    # ä½¿ç”¨ params ç·¨ç¢¼é—œéµå­—
    encoded_keyword = requests.utils.quote(keyword)
    
    # é€™æ˜¯æ¨™æº–çš„ Google News RSS æ ¼å¼
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    feed = feedparser.parse(rss_url)
    
    results = []
    # åªå–å‰ limit ç­†
    for entry in feed.entries[:limit]:
        results.append({
            "title": entry.title,
            "link": entry.link, # é€™è£¡æ‹¿åˆ°çš„æ˜¯ Google çš„è½‰å€é€£çµ
            "published": entry.published
        })
    return results

# --- 4. ä¸»ç¨‹å¼ä»‹é¢ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ã€OpenAI...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æœå°‹')

if submit_button and keyword:
    st.divider()
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    progress_text.text(f"ğŸ” æ­£åœ¨ Google News æœå°‹ã€Œ{keyword}ã€...")
    
    # 1. ç²å– RSS åˆ—è¡¨
    news_items = search_google_news_rss(keyword, limit=5)
    
    if not news_items:
        st.warning("Google News æ‰¾ä¸åˆ°ç›¸é—œæ–°èï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚")
        progress_bar.empty()
    else:
        results_data = []
        total = len(news_items)
        
        for i, item in enumerate(news_items):
            progress_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): è§£ç¢¼é€£çµä¸¦æ‘˜è¦ä¸­... {item['title'][:10]}...")
            progress_bar.progress((i + 1) / total)
            
            # 2. çˆ¬å–èˆ‡æ‘˜è¦ (åŒ…å«é€£çµè§£ç¢¼)
            summary, real_url = extract_and_process(item['link'])
            
            results_data.append({
                "æ–°èæ¨™é¡Œ": item['title'],
                "AI é‡é»æ‘˜è¦": summary,
                "çœŸå¯¦é€£çµ": real_url
            })
        
        progress_bar.empty()
        progress_text.empty()
        
        st.success("âœ… å®Œæˆï¼")
        
        df = pd.DataFrame(results_data)
        st.dataframe(
            df,
            column_config={
                "çœŸå¯¦é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”— å‰å¾€é–±è®€"),
                "AI é‡é»æ‘˜è¦": st.column_config.TextColumn("AI é‡é»æ‘˜è¦", width="large"),
            },
            hide_index=True,
            use_container_width=True
        )