import streamlit as st
import pandas as pd
import requests
import feedparser
from newspaper import Article, Config
import jieba
import nltk
from bs4 import BeautifulSoup
import re

# --- 1. NLTK å¼·åˆ¶ä¿®å¾© ---
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="Google News æ‘˜è¦ (é›²ç«¯ä¿®æ­£ç‰ˆ)", page_icon="ğŸ›¡ï¸", layout="wide")
st.title("ğŸ›¡ï¸ Google News AI æ‘˜è¦ (é›²ç«¯é˜»æ“‹çªç ´ç‰ˆ)")
st.markdown("é‡å° **Streamlit Cloud IP** è¢« Google è­˜åˆ¥ç‚ºæ©Ÿå™¨äººçš„å•é¡Œé€²è¡Œä¿®å¾©ã€‚")

# --- 3. æ ¸å¿ƒåŠŸèƒ½ï¼šå¼·åŠ›é€£çµè§£æ ---

def get_real_url(google_url):
    """
    é‡å° Streamlit Cloud ç’°å¢ƒçš„å¼·åŠ›è§£ç¢¼ã€‚
    ç•¶ requests æ‹¿åˆ° Google çš„ä¸­è½‰é é¢ (Consent Page) æ™‚ï¼Œ
    ç›´æ¥ç”¨ BeautifulSoup æš´åŠ›æŒ–å‡ºè£¡é¢çš„ç›®æ¨™é€£çµã€‚
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'Referer': 'https://news.google.com/'
        }
        
        # 1. ç™¼é€è«‹æ±‚
        response = requests.get(google_url, headers=headers, timeout=15)
        
        # 2. å¦‚æœç¶²å€å·²ç¶“è·³è½‰å‡º googleï¼Œç›´æ¥å›å‚³
        if 'news.google.com' not in response.url and 'google.com' not in response.url:
            return response.url

        # 3. å¦‚æœé‚„åœ¨ Googleï¼Œä»£è¡¨è¢«æ“‹ä½äº†ã€‚è§£æ HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Google ä¸­è½‰é é€šå¸¸æœ‰ä¸€å€‹ä¸»è¦çš„é€£çµå¯«è‘— "Opening..." æˆ–éš±è—åœ¨ JS ä¸­
        # æ–¹æ³• A: æ‰¾å°‹é é¢ä¸­ä¸»è¦çš„ <a> æ¨™ç±¤ (é€šå¸¸æ˜¯ç¬¬ä¸€å€‹é Google çš„é€£çµ)
        links = soup.find_all('a')
        for link in links:
            href = link.get('href')
            if href and href.startswith('http') and 'google.com' not in href:
                return href

        # æ–¹æ³• B: æœå°‹ JavaScript ä¸­çš„è·³è½‰é€£çµ
        # é¡ä¼¼ window.location.replace("https://...")
        match = re.search(r'window\.location\.replace\("(.*?)"\)', response.text)
        if match:
            return match.group(1)
            
        # æ–¹æ³• C: æœå°‹ <noscript> å€å¡Šä¸­çš„é€£çµ
        noscript = soup.find('noscript')
        if noscript:
            link = noscript.find('a')
            if link and link.get('href'):
                return link.get('href')

        # å¦‚æœéƒ½å¤±æ•—ï¼Œå›å‚³åŸå§‹é€£çµ (é›–ç„¶å¯èƒ½æœƒæ‘˜è¦å¤±æ•—ï¼Œä½†æ²’è¾¦æ³•äº†)
        return google_url
        
    except Exception as e:
        return google_url

def sumy_summarize(text, sentence_count=3):
    try:
        if not text: return "ç„¡å…§å®¹"
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        
        result = ""
        for sentence in summary_sentences:
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
        return result
    except Exception as e:
        return f"æ‘˜è¦é‹ç®—éŒ¯èª¤: {e}"

def extract_and_process(google_url):
    try:
        # æ­¥é©Ÿ 1: ç²å–çœŸå¯¦ç¶²å€ (é—œéµæ­¥é©Ÿ)
        real_url = get_real_url(google_url)
        
        # å¦‚æœè§£ç¢¼å¾Œé‚„æ˜¯ google ç¶²å€ï¼Œé¡¯ç¤ºè­¦å‘Š
        if "google.com" in real_url:
            return "âš ï¸ ç„¡æ³•ç©¿é€ Google è½‰å€é  (IP è¢«é˜»æ“‹)", real_url

        # æ­¥é©Ÿ 2: çˆ¬å–å…§å®¹
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        config.request_timeout = 10
        
        article = Article(real_url, config=config)
        article.download()
        article.parse()
        
        # æ­¥é©Ÿ 3: æª¢æŸ¥é•·åº¦
        if len(article.text) < 50:
             if article.meta_description and len(article.meta_description) > 10:
                 return f"ğŸ“Œ (ä¾†æºç°¡ä»‹) {article.meta_description}", real_url
             return "âš ï¸ ç¶²ç«™é˜»æ“‹çˆ¬èŸ² (ç„¡å…§å®¹)", real_url

        summary = sumy_summarize(article.text, sentence_count=3)
        return summary, real_url
        
    except Exception as e:
        return f"âŒ è™•ç†éŒ¯èª¤: {str(e)}", google_url

def search_google_news_rss(keyword, limit=5):
    encoded_keyword = requests.utils.quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    feed = feedparser.parse(rss_url)
    
    results = []
    for entry in feed.entries[:limit]:
        results.append({
            "title": entry.title,
            "link": entry.link,
            "published": entry.published
        })
    return results

# --- 4. ä¸»ç¨‹å¼ä»‹é¢ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æœå°‹')

if submit_button and keyword:
    st.divider()
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    progress_text.text(f"ğŸ” æ­£åœ¨ Google News æœå°‹ã€Œ{keyword}ã€...")
    news_items = search_google_news_rss(keyword, limit=5)
    
    if not news_items:
        st.warning("æ‰¾ä¸åˆ°ç›¸é—œæ–°èã€‚")
        progress_bar.empty()
    else:
        results_data = []
        total = len(news_items)
        
        for i, item in enumerate(news_items):
            progress_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): å˜—è©¦ç ´è§£è½‰å€... {item['title'][:10]}")
            progress_bar.progress((i + 1) / total)
            
            # å‘¼å«è™•ç†å‡½å¼
            summary, real_url = extract_and_process(item['link'])
            
            results_data.append({
                "æ–°èæ¨™é¡Œ": item['title'],
                "AI é‡é»æ‘˜è¦": summary,
                "çœŸå¯¦é€£çµ": real_url
            })
        
        progress_bar.empty()
        progress_text.empty()
        
        st.success("âœ… å®Œæˆï¼å¦‚æœæ‘˜è¦é¡¯ç¤º IP é˜»æ“‹ï¼Œå»ºè­°é‡æ–°æ•´ç†æˆ–ç¨å¾Œå†è©¦ã€‚")
        
        df = pd.DataFrame(results_data)
        st.dataframe(
            df,
            column_config={
                "çœŸå¯¦é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”— å‰å¾€é–±è®€"),
                "AI é‡é»æ‘˜è¦": st.column_config.TextColumn("AI é‡é»æ‘˜è¦", width="large"),
            },
            hide_index=True,
            use_container_width=True
        )