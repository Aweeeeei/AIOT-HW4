import streamlit as st
import pandas as pd
import requests
from newspaper import Article, Config
import jieba
import nltk
from datetime import datetime, timedelta

# --- 1. NLTK è‡ªå‹•ä¿®å¾© ---
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="GNews AI æ–°èåŠ©æ‰‹", page_icon="ğŸ“¡", layout="wide")
st.title("ğŸ“¡ GNews AI æ–°èæ‘˜è¦ (åƒæ•¸å„ªåŒ–ç‰ˆ)")
st.markdown("ä¾†æºï¼š**GNews API** | ç­–ç•¥ï¼š**æŒ‰æ™‚é–“æ’åº (publishedAt)** + **æ”¾å¯¬åœ°å€**")

# --- 3. API Key ---
GNEWS_API_KEY = "b8bba61d5cec4532cc9b3630311eed30"

# --- 4. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def sumy_summarize(text, sentence_count=3):
    try:
        if not text: return "ç„¡å…§å®¹"
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        result = ""
        for sentence in summary_sentences:
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
        return result
    except Exception as e:
        return f"æ‘˜è¦éŒ¯èª¤: {e}"

def extract_and_process(url):
    try:
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        config.request_timeout = 10
        article = Article(url, config=config)
        article.download()
        article.parse()
        if len(article.text) < 50:
             return "âš ï¸ ç¶²ç«™å…§å®¹éçŸ­ (å»ºè­°é»æ“Šé€£çµé–±è®€)", url
        summary = sumy_summarize(article.text, sentence_count=3)
        return summary, url
    except Exception as e:
        return f"âŒ æŠ“å–éŒ¯èª¤: {str(e)}", url

def search_gnews(keyword, limit=5):
    """
    ä½¿ç”¨ GNews API é€²è¡Œæœå°‹ (å·²ç§»é™¤åœ‹å®¶é™åˆ¶ï¼Œå¼·åˆ¶æŒ‰æ™‚é–“æ’åº)
    """
    try:
        url = "https://gnews.io/api/v4/search"
        
        # --- é—œéµä¿®æ­£ï¼šè¨ˆç®— 28 å¤©å‰çš„æ™‚é–“å­—ä¸² (ISO 8601 æ ¼å¼) ---
        past_date = datetime.utcnow() - timedelta(days=28)
        from_date_str = past_date.strftime('%Y-%m-%dT%H:%M:%SZ')

        params = {
            'q': keyword,
            'token': GNEWS_API_KEY,
            'lang': 'zh',           # åªé™åˆ¶ç¹é«”ä¸­æ–‡
            # 'country': 'tw',      # ç§»é™¤åœ‹å®¶é™åˆ¶ï¼Œé¿å…ç¯„åœéçª„
            'max': limit,
            'sortby': 'publishedAt', # å¼·åˆ¶æŒ‰æ™‚é–“æ’åºï¼(é—œéµ)
            'from': from_date_str    # é–å®šæœ€è¿‘ä¸€å€‹æœˆ
        }
        
        response = requests.get(url, params=params)
        data = response.json()
        
        # --- DEBUG å€å¡Šï¼šè®“ä½¿ç”¨è€…çœ‹åˆ°åŸå§‹å›å‚³ ---
        with st.expander("ğŸ” é»æ“ŠæŸ¥çœ‹ API åŸå§‹å›å‚³è³‡æ–™ (Debug)", expanded=True):
            st.json(data)
        # -------------------------------------

        if response.status_code != 200:
            st.error(f"API ç‹€æ…‹ç¢¼éŒ¯èª¤: {response.status_code}")
            return []

        articles = data.get('articles', [])
        return articles

    except Exception as e:
        st.error(f"é€£ç·šéŒ¯èª¤: {e}")
        return []

# --- 5. ä¸»ç¨‹å¼ä»‹é¢ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ã€AI...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æœå°‹')

if submit_button and keyword:
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    progress_text.text(f"ğŸ” æ­£åœ¨å‘¼å« GNews API...")
    
    # 1. å‘¼å« API
    articles = search_gnews(keyword, limit=5)
    
    if not articles:
        st.warning("âš ï¸ æœå°‹çµæœç‚ºç©ºã€‚è«‹æŸ¥çœ‹ä¸Šæ–¹çš„ Debug JSON ç¢ºèªåŸå› ã€‚")
        progress_bar.empty()
    else:
        results_data = []
        total = len(articles)
        
        for i, item in enumerate(articles):
            title = item.get('title')
            url = item.get('url')
            api_desc = item.get('description', '')
            
            progress_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): {title[:15]}...")
            progress_bar.progress((i + 1) / total)
            
            summary, real_url = extract_and_process(url)
            
            if summary.startswith("âš ï¸") or summary.startswith("âŒ"):
                summary = f"ğŸ“Œ (API æ‘˜è¦) {api_desc}"
            
            results_data.append({
                "æ¨™é¡Œ": title,
                "AI æ‘˜è¦": summary,
                "æ™‚é–“": item.get('publishedAt', '')[:10],
                "é€£çµ": real_url
            })
        
        progress_bar.empty()
        progress_text.empty()
        
        st.success(f"âœ… å®Œæˆï¼å…±æœå°‹åˆ° {total} ç¯‡æ–°èã€‚")
        df = pd.DataFrame(results_data)
        st.dataframe(
            df, 
            column_config={"é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”— é–±è®€")},
            hide_index=True,
            use_container_width=True
        )