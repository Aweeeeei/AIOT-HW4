import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
from newspaper import Article, Config
import jieba
import nltk

# --- 1. NLTK è‡ªå‹•ä¿®å¾© (é›²ç«¯ç’°å¢ƒå¿…å‚™) ---
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="Yahoo è²¡ç¶“æ–°èæ‘˜è¦", page_icon="ğŸ“ˆ", layout="wide")
st.title("ğŸ“ˆ Yahoo è²¡ç¶“æ–°è AI æ‘˜è¦")
st.markdown("ä¾†æºï¼š**Yahoo è‚¡å¸‚** | æŠ€è¡“ï¼š**LSA æ¼”ç®—æ³•** + **Python çˆ¬èŸ²**")

# --- 3. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def sumy_summarize(text, sentence_count=3):
    """ä½¿ç”¨ Sumy + Jieba é€²è¡Œä¸­æ–‡èƒå–å¼æ‘˜è¦"""
    try:
        if not text: return "ç„¡å…§å®¹"
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        
        result = ""
        for sentence in summary_sentences:
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
        return result
    except Exception as e:
        return f"æ‘˜è¦éŒ¯èª¤: {e}"

def extract_and_process(url):
    """
    æŠ“å–ä¸¦æ‘˜è¦ Yahoo æ–°è
    Yahoo çš„é€£çµé€šå¸¸å¾ˆä¹¾æ·¨ï¼Œç›´æ¥çˆ¬å–å³å¯ã€‚
    """
    try:
        # è¨­å®šå½è£ç€è¦½å™¨
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        config.request_timeout = 10
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        
        # æª¢æŸ¥å…§å®¹é•·åº¦
        if len(article.text) < 50:
             return "âš ï¸ å…§å®¹éçŸ­æˆ–éæ–°èæ ¼å¼ (å¯èƒ½æ˜¯å½±ç‰‡æˆ–åœ–è¡¨)", url

        # åŸ·è¡Œæ‘˜è¦
        summary = sumy_summarize(article.text, sentence_count=3)
        return summary, url
        
    except Exception as e:
        return f"âŒ è™•ç†éŒ¯èª¤: {str(e)}", url

def scrape_yahoo_finance(keyword, limit=5):
    """
    çˆ¬å– Yahoo è‚¡å¸‚æœå°‹çµæœ
    åƒè€ƒè‡ª: LearnCodeWithMike (é‡å°æœå°‹é é¢æ”¹å¯«)
    """
    results = []
    try:
        # Yahoo è‚¡å¸‚æœå°‹ URL
        url = f"https://tw.stock.yahoo.com/search?p={keyword}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        
        # ç­–ç•¥ï¼šæŠ“å–é é¢ä¸­æ‰€æœ‰çœ‹èµ·ä¾†åƒæ–°èçš„é€£çµ
        # Yahoo æœå°‹é é¢çµæ§‹æ¯”è¼ƒé›œï¼Œæˆ‘å€‘æ‰¾ href åŒ…å« "/news/" çš„é€£çµ
        # ä¸¦ä¸”æ’é™¤é‡è¤‡çš„
        
        seen_links = set()
        count = 0
        
        # æŠ“å–æ‰€æœ‰é€£çµ
        links = soup.find_all('a', href=True)
        
        for link in links:
            href = link['href']
            title = link.get_text().strip()
            
            # ç¯©é¸æ¢ä»¶ï¼š
            # 1. é€£çµåŒ…å« '/news/' (ä»£è¡¨æ˜¯æ–°è)
            # 2. æ¨™é¡Œé•·åº¦å¤§æ–¼ 10 (éæ¿¾æ‰ç„¡æ„ç¾©çš„æŒ‰éˆ•)
            # 3. ä¸åœ¨å·²æŠ“å–æ¸…å–®ä¸­
            if '/news/' in href and len(title) > 10 and href not in seen_links:
                
                # è™•ç†ç›¸å°è·¯å¾‘ (é›–ç„¶ Yahoo é€šå¸¸çµ¦çµ•å°è·¯å¾‘ï¼Œä¿éšªèµ·è¦‹)
                if not href.startswith('http'):
                    href = 'https://tw.stock.yahoo.com' + href
                
                results.append({
                    "title": title,
                    "link": href
                })
                seen_links.add(href)
                count += 1
                
                if count >= limit:
                    break
                    
    except Exception as e:
        st.error(f"Yahoo çˆ¬èŸ²ç™¼ç”ŸéŒ¯èª¤: {e}")
        
    return results

# --- 4. ä¸»ç¨‹å¼ä»‹é¢ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥é‡‘èé—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ã€ETFã€é«˜è‚¡æ¯...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æœå°‹ Yahoo')

if submit_button and keyword:
    st.divider()
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    progress_text.text(f"ğŸ” æ­£åœ¨çˆ¬å– Yahoo è‚¡å¸‚ï¼šã€Œ{keyword}ã€...")
    
    # 1. çˆ¬å–
    news_items = scrape_yahoo_finance(keyword, limit=5)
    
    if not news_items:
        st.warning("æ‰¾ä¸åˆ°ç›¸é—œæ–°èï¼ŒYahoo æœå°‹é é¢çµæ§‹å¯èƒ½å·²æ›´æ–°ï¼Œæˆ–ç„¡ç›¸é—œè³‡æ–™ã€‚")
        progress_bar.empty()
    else:
        results_data = []
        total = len(news_items)
        
        for i, item in enumerate(news_items):
            progress_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): {item['title'][:15]}...")
            progress_bar.progress((i + 1) / total)
            
            # 2. æ‘˜è¦
            summary, real_url = extract_and_process(item['link'])
            
            results_data.append({
                "æ–°èæ¨™é¡Œ": item['title'],
                "AI é‡é»æ‘˜è¦": summary,
                "é€£çµ": real_url
            })
        
        progress_bar.empty()
        progress_text.empty()
        
        st.success(f"âœ… å®Œæˆï¼å…±æ‰¾åˆ° {total} ç¯‡ç›¸é—œå ±å°ã€‚")
        
        df = pd.DataFrame(results_data)
        st.dataframe(
            df,
            column_config={
                "é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”— å‰å¾€ Yahoo"),
                "AI é‡é»æ‘˜è¦": st.column_config.TextColumn("AI é‡é»æ‘˜è¦", width="large"),
            },
            hide_index=True,
            use_container_width=True
        )