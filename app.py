import streamlit as st
import pandas as pd
import requests
import feedparser
from newspaper import Article, Config
import jieba
import nltk # å¼•å…¥ nltk

# --- NLTK è³‡æ–™ä¸‹è¼‰ä¿®æ­£å€ (è§£æ±ºæ‘˜è¦éŒ¯èª¤) ---
# Streamlit Cloud é è¨­æ²’æœ‰é€™äº›è³‡æ–™ï¼Œå¿…é ˆåœ¨åŸ·è¡Œæ™‚ä¸‹è¼‰
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt', quiet=True)
    nltk.download('punkt_tab', quiet=True)

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="æ–°èæ‘˜è¦åŠ©æ‰‹", page_icon="ğŸ“°", layout="wide")
st.title("ğŸ“° AI æ–°èæœå°‹èˆ‡æ‘˜è¦ (Bing ä¾†æºç©©å®šç‰ˆ)")
st.markdown("ä½¿ç”¨ **Bing News RSS** æœå°‹ï¼Œæ­é… **LSA æ¼”ç®—æ³•** é€²è¡Œæ¥µé€Ÿæ‘˜è¦ã€‚")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def sumy_summarize(text, sentence_count=3):
    """ä½¿ç”¨ Sumy + Jieba é€²è¡Œä¸­æ–‡èƒå–å¼æ‘˜è¦"""
    try:
        if not text: return "ç„¡å…§å®¹"
        
        # ä¸­æ–‡æ–·è©è™•ç†
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        
        # åˆå§‹åŒ–æ‘˜è¦å™¨
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        
        # çµ„åˆçµæœ
        result = ""
        for sentence in summary_sentences:
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
        return result
    except Exception as e:
        return f"æ‘˜è¦éŒ¯èª¤: {e}"

def extract_and_process(url):
    """
    æŠ“å–ä¸¦æ‘˜è¦
    """
    try:
        # è¨­å®šå½è£ç€è¦½å™¨ User-Agent
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
        config.request_timeout = 10
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        
        # å¦‚æœæ­£æ–‡å¤ªçŸ­ï¼Œæ”¹æŠ“ Meta Description
        if len(article.text) < 50:
             if article.meta_description and len(article.meta_description) > 10:
                 return f"ğŸ“Œ (ä¾†æºç°¡ä»‹) {article.meta_description}"
             return "âš ï¸ ç„¡æ³•æŠ“å–å…§å®¹ (ç¶²ç«™é˜»æ“‹çˆ¬èŸ²)"

        # åŸ·è¡Œæ‘˜è¦
        summary = sumy_summarize(article.text, sentence_count=3)
        return summary
        
    except Exception as e:
        return f"âŒ è™•ç†éŒ¯èª¤: {str(e)}"

def search_bing_rss(keyword, limit=5):
    """
    ä½¿ç”¨ Bing News RSS é€²è¡Œæœå°‹
    """
    rss_url = f"https://www.bing.com/news/search?q={keyword}&format=rss"
    feed = feedparser.parse(rss_url)
    
    results = []
    # é€™è£¡æœƒå›å‚³å¯¦éš›æ‰¾åˆ°çš„æ•¸é‡ï¼Œæœ€å¤š limit ç­†
    for entry in feed.entries[:limit]:
        results.append({
            "title": entry.title,
            "link": entry.link,
            "published": entry.published if 'published' in entry else "Unknown"
        })
        
    return results, len(feed.entries) # å›å‚³è³‡æ–™èˆ‡ç¸½æœå°‹æ•¸

# --- 3. ä¸»ç¨‹å¼é‚è¼¯ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚ï¼šOpenAI, å°ç©é›»...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æœå°‹')

if submit_button and keyword:
    st.divider()
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    progress_text.text(f"ğŸ” æ­£åœ¨é€é Bing æœå°‹ã€Œ{keyword}ã€...")
    
    # åŸ·è¡Œæœå°‹
    news_items, total_found = search_bing_rss(keyword, limit=5)
    
    if not news_items:
        st.warning(f"Bing æ‰¾ä¸åˆ°ç›¸é—œæ–°è (æœå°‹å›å‚³ 0 ç­†)ã€‚è«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚")
        progress_bar.empty()
    else:
        # é¡¯ç¤ºå¯¦éš›æ‰¾åˆ°çš„æ•¸é‡ï¼Œè®“ä½ çŸ¥é“ç‚ºä»€éº¼åªæœ‰ 2 ç­†
        st.info(f"Bing å…±å›å‚³ {total_found} ç­†ç›¸é—œæ–°èï¼Œç³»çµ±å°‡è™•ç†å‰ {len(news_items)} ç­†ã€‚")
        
        results_data = []
        process_count = len(news_items)
        
        for i, item in enumerate(news_items):
            progress_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{process_count}): {item['title']} ...")
            progress_bar.progress((i + 1) / process_count)
            
            summary = extract_and_process(item['link'])
            
            results_data.append({
                "æ–°èæ¨™é¡Œ": item['title'],
                "AI é‡é»æ‘˜è¦": summary,
                "é€£çµ": item['link']
            })
        
        progress_bar.empty()
        progress_text.empty()
        
        st.success("âœ… å®Œæˆï¼")
        
        df = pd.DataFrame(results_data)
        st.dataframe(
            df,
            column_config={
                "é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”— å‰å¾€é–±è®€"),
                "AI é‡é»æ‘˜è¦": st.column_config.TextColumn("AI é‡é»æ‘˜è¦", width="large"),
            },
            hide_index=True,
            use_container_width=True
        )