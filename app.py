import streamlit as st
import pandas as pd
import requests
import feedparser
from newspaper import Article
import time

# --- å¼•å…¥è¼•é‡åŒ– NLP å¥—ä»¶ ---
import jieba
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer # ä½¿ç”¨ LSA æ¼”ç®—æ³•
# ä¹Ÿå¯ä»¥æ›æˆ LexRankSummarizerï¼Œæ•ˆæœä¹Ÿä¸éŒ¯

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="æ¥µé€Ÿæ–°èæ‘˜è¦åŠ©æ‰‹", page_icon="âš¡", layout="wide")
st.title("âš¡ æ¥µé€Ÿç‰ˆ Google æ–°èæ‘˜è¦ (CPU Friendly)")
st.markdown("ä½¿ç”¨ **LSA æ¼”ç®—æ³•** å–ä»£æ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œå¯¦ç¾ **æ¯«ç§’ç´š** çš„å¿«é€Ÿæ‘˜è¦ã€‚")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def search_google_news_rss(keyword, limit=5):
    """(ç¶­æŒä¸è®Š) ä½¿ç”¨ Google News RSS Feed é€²è¡Œæœå°‹"""
    encoded_keyword = requests.utils.quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    feed = feedparser.parse(rss_url)
    
    news_list = []
    for entry in feed.entries[:limit]:
        real_url = get_actual_url(entry.link)
        if real_url:
            news_list.append({
                "title": entry.title,
                "link": real_url,
                "published": entry.published
            })
    return news_list

def get_actual_url(google_url):
    """(ç¶­æŒä¸è®Š) è§£æçœŸå¯¦ URL"""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.head(google_url, allow_redirects=True, headers=headers, timeout=5)
        return response.url
    except:
        return None

def sumy_summarize(text, sentence_count=3):
    """
    ä½¿ç”¨ Sumy + Jieba é€²è¡Œä¸­æ–‡èƒå–å¼æ‘˜è¦
    """
    try:
        # 1. ä¸­æ–‡å‰è™•ç†ï¼šå› ç‚º sumy é è¨­æ˜¯ä»¥ç©ºç™½åˆ†è©ï¼Œä¸­æ–‡éœ€è¦å…ˆç”¨ jieba åˆ‡é–‹
        # ä¾‹å¦‚ï¼š"ä»Šå¤©å¤©æ°£å¾ˆå¥½" -> "ä»Šå¤© å¤©æ°£ å¾ˆå¥½"
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        
        # 2. å»ºç«‹ Parser
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        # é€™è£¡å€Ÿç”¨ english tokenizerï¼Œå› ç‚ºæˆ‘å€‘å·²ç¶“æ‰‹å‹•ç”¨ç©ºç™½åˆ‡é–‹ä¸­æ–‡è©å½™äº†
        
        # 3. åˆå§‹åŒ–æ‘˜è¦å™¨ (LSA)
        summarizer = LsaSummarizer() 
        
        # 4. åŸ·è¡Œæ‘˜è¦ï¼Œå–å‡ºæœ€é‡è¦çš„ N å€‹å¥å­
        summary_sentences = summarizer(parser.document, sentence_count)
        
        # 5. çµ„åˆçµæœ (é‚„åŸæˆä¸å¸¶ç©ºç™½çš„ä¸­æ–‡)
        result = ""
        for sentence in summary_sentences:
            # sumy çš„ sentence ç‰©ä»¶è½‰å­—ä¸²å¾Œæœƒæœ‰ç©ºç™½ï¼Œæˆ‘å€‘æŠŠå®ƒå»æ‰ (ç°¡å–®è™•ç†)
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
            
        return result
        
    except Exception as e:
        return f"æ‘˜è¦éŒ¯èª¤: {e}"

def extract_and_process(url):
    """æŠ“å–ä¸¦æ‘˜è¦"""
    try:
        article = Article(url)
        article.download()
        article.parse()
        
        if len(article.text) < 50:
             return "æ–‡ç« å…§å®¹å¤ªçŸ­"

        # ä½¿ç”¨ Sumy é€²è¡Œæ‘˜è¦ (åªè¦ 0.01ç§’)
        summary = sumy_summarize(article.text, sentence_count=3)
        return summary
        
    except Exception as e:
        return f"ç„¡æ³•è®€å–: {str(e)}"

# --- 3. ä¸»ç¨‹å¼é‚è¼¯ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ã€è¼é”...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æ¥µé€Ÿæœå°‹')

if submit_button and keyword:
    st.divider()
    
    # é€™è£¡ä¸éœ€è¦é€²åº¦æ¢äº†ï¼Œå› ç‚ºé€Ÿåº¦æœƒéå¸¸å¿«
    with st.spinner('æ­£åœ¨å…‰é€Ÿæœå°‹èˆ‡æ‘˜è¦ä¸­...'):
        news_items = search_google_news_rss(keyword)
        
        results_data = []
        for item in news_items:
            # æŠ“å– + æ‘˜è¦
            summary = extract_and_process(item['link'])
            
            results_data.append({
                "æ–°èæ¨™é¡Œ": item['title'],
                "é‡é»æ‘˜è¦ (LSAèƒå–)": summary,
                "åŸå§‹é€£çµ": item['link']
            })
            
    # ç›´æ¥é¡¯ç¤ºçµæœ
    if results_data:
        st.success(f"å·²å®Œæˆï¼å…±æ‰¾åˆ° {len(results_data)} ç¯‡æ–°èã€‚")
        df = pd.DataFrame(results_data)
        st.dataframe(
            df,
            column_config={
                "åŸå§‹é€£çµ": st.column_config.LinkColumn("åŸå§‹é€£çµ", display_text="ğŸ”— é–±è®€åŸæ–‡"),
                "é‡é»æ‘˜è¦ (LSAèƒå–)": st.column_config.TextColumn("é‡é»æ‘˜è¦", width="large")
            },
            hide_index=True,
            use_container_width=True
        )
    else:
        st.warning("æ‰¾ä¸åˆ°ç›¸é—œæ–°èã€‚")