import streamlit as st
import pandas as pd
import requests
from newspaper import Article, Config
import nltk
from concurrent.futures import ThreadPoolExecutor, as_completed
from deep_translator import GoogleTranslator # æ–°å¢žç¿»è­¯å·¥å…·

# --- 1. NLTK è‡ªå‹•ä¿®å¾© ---
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="Massive é‡‘èžæ–°èž (ä¸­è­¯ç‰ˆ)", page_icon="ðŸ¦", layout="wide")
st.title("ðŸ¦ Massive ç¾Žè‚¡æ–°èžæ‘˜è¦")
st.markdown("ä¾†æºï¼š**Massive (Polygon)** | æ ¸å¿ƒï¼š**LSA æ‘˜è¦** + **è‡ªå‹•ç¿»è­¯** + **å¤šåŸ·è¡Œç·’åŠ é€Ÿ**")
st.info("ðŸ’¡ æç¤ºï¼šè¼¸å…¥ç¾Žè‚¡ä»£è™Ÿ (ä¾‹å¦‚ **TSM**, **NVDA**, **AAPL**)")

# --- 3. API Key ---
MASSIVE_API_KEY = "vMnBeXpL5XKK4G1nuf2jmXR9B2wXuC15"

# --- 4. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def translate_to_chinese(text):
    """
    ä½¿ç”¨ deep-translator å¿«é€Ÿå°‡è‹±æ–‡è½‰ä¸­æ–‡
    """
    try:
        # source='auto' è‡ªå‹•åµæ¸¬, target='zh-TW' ç¹é«”ä¸­æ–‡
        translated = GoogleTranslator(source='auto', target='zh-TW').translate(text)
        return translated
    except Exception:
        return text # å¦‚æžœç¿»è­¯å¤±æ•—ï¼Œå›žå‚³åŽŸæ–‡

def sumy_summarize(text, sentence_count=3):
    try:
        if not text: return "ç„¡å…§å®¹"
        
        # è‹±æ–‡æ–·è©ž (Massive ä¾†æºä¸»è¦æ˜¯è‹±æ–‡)
        parser = PlaintextParser.from_string(text, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        
        # çµ„åˆè‹±æ–‡æ‘˜è¦
        english_summary = " ".join([str(sentence) for sentence in summary_sentences])
        
        # --- ç¿»è­¯æˆä¸­æ–‡ ---
        if english_summary:
            chinese_summary = translate_to_chinese(english_summary)
            return chinese_summary
        
        return "ç„¡æ³•ç”¢ç”Ÿæ‘˜è¦"
    except Exception as e:
        return f"æ‘˜è¦éŒ¯èª¤: {e}"

def extract_and_process(item):
    """
    å–®ç¯‡æ–‡ç« è™•ç†æµç¨‹ (ä¸‹è¼‰ -> æ‘˜è¦ -> ç¿»è­¯)
    """
    url = item.get('article_url')
    title = item.get('title')
    publisher = item.get('publisher', {}).get('name', 'Unknown')
    pub_time = item.get('published_utc', '')[:10]
    
    try:
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36'
        config.request_timeout = 5 # ç¸®çŸ­è¶…æ™‚è¨­å®šä»¥åŠ å¿«é€Ÿåº¦
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        
        if len(article.text) < 50:
             # å¦‚æžœå…§æ–‡å¤ªçŸ­ï¼Œå˜—è©¦ç¿»è­¯ API çµ¦çš„ description
             desc = item.get('description', '')
             if desc:
                 return {
                     "æ–°èžæ¨™é¡Œ": title,
                     "åª’é«”ä¾†æº": publisher,
                     "AI é‡é»žæ‘˜è¦": f"ðŸ“Œ (å®˜æ–¹æ‘˜è¦) {translate_to_chinese(desc)}",
                     "ç™¼å¸ƒæ™‚é–“": pub_time,
                     "é€£çµ": url
                 }
             return None

        # åŸ·è¡Œæ‘˜è¦ + ç¿»è­¯
        summary = sumy_summarize(article.text, sentence_count=3)
        
        return {
            "æ–°èžæ¨™é¡Œ": title,
            "åª’é«”ä¾†æº": publisher,
            "AI é‡é»žæ‘˜è¦": summary,
            "ç™¼å¸ƒæ™‚é–“": pub_time,
            "é€£çµ": url
        }
        
    except Exception as e:
        return None

def search_massive_news(ticker, limit=5):
    try:
        url = "https://api.polygon.io/v2/reference/news"
        params = {
            'ticker': ticker.upper(),
            'limit': limit,
            'apiKey': MASSIVE_API_KEY,
            'sort': 'published_utc',
            'order': 'desc'
        }
        response = requests.get(url, params=params, timeout=10)
        data = response.json()
        return data.get('results', [])
    except Exception as e:
        st.error(f"é€£ç·šéŒ¯èª¤: {e}")
        return []

# --- 5. ä¸»ç¨‹å¼ä»‹é¢ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥ç¾Žè‚¡ä»£è™Ÿ (Ticker)", value="TSM")
    with col2:
        submit_button = st.form_submit_button(label='ðŸš€ æœå°‹')

if submit_button and keyword:
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    progress_text.text(f"ðŸ” æœå°‹ä¸­...")
    
    articles = search_massive_news(keyword, limit=5)
    
    if not articles:
        st.warning(f"æ‰¾ä¸åˆ° {keyword.upper()} çš„æ–°èžã€‚")
        progress_bar.empty()
    else:
        results_data = []
        total = len(articles)
        
        # --- å¹³è¡Œè™•ç† (Parallel Processing) ---
        # é€™æœƒåŒæ™‚é–‹å•Ÿ 5 å€‹åŸ·è¡Œç·’åŽ»ä¸‹è¼‰ã€æ‘˜è¦ã€ç¿»è­¯ï¼Œé€Ÿåº¦æå‡ 5 å€
        with ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤ä»»å‹™
            future_to_article = {executor.submit(extract_and_process, item): item for item in articles}
            
            completed_count = 0
            for future in as_completed(future_to_article):
                result = future.result()
                if result:
                    results_data.append(result)
                
                completed_count += 1
                progress_text.text(f"æ­£åœ¨è™•ç† ({completed_count}/{total})...")
                progress_bar.progress(completed_count / total)
        
        # æŽ’åºå›žåŽŸæœ¬çš„æ™‚é–“é †åº (å› ç‚ºå¹³è¡Œè™•ç†å®Œæˆé †åºä¸ä¸€å®š)
        # ç°¡å–®è§£æ³•ï¼šé€™è£¡ä¸ç‰¹åˆ¥æŽ’ï¼Œå› ç‚ºå·®ç•°ä¸å¤§ï¼Œè‹¥è¦æŽ’å¯ä¾æ™‚é–“æ¬„ä½ sort
        
        progress_bar.empty()
        progress_text.empty()
        
        if results_data:
            st.success(f"âœ… å®Œæˆï¼(å«è‡ªå‹•ç¿»è­¯)")
            df = pd.DataFrame(results_data)
            st.dataframe(
                df, 
                column_config={
                    "é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ðŸ”— é–±è®€"),
                    "AI é‡é»žæ‘˜è¦": st.column_config.TextColumn("AI é‡é»žæ‘˜è¦", width="large")
                },
                hide_index=True,
                use_container_width=True
            )
        else:
            st.warning("é›–æœ‰æ‰¾åˆ°æ–°èžæ¨™é¡Œï¼Œä½†å…§å®¹æŠ“å–å¤±æ•— (å¯èƒ½æ˜¯ä»˜è²»ç‰†æˆ–é˜»æ“‹)ã€‚")