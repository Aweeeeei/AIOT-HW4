import streamlit as st
import pandas as pd
import requests
import feedparser
from newspaper import Article, Config
import jieba
import nltk
from bs4 import BeautifulSoup # éœ€ç”¨åˆ° BS4 ä¾†è§£æè½‰å€é 
import re

# --- 1. NLTK è‡ªå‹•ä¿®å¾© (Streamlit Cloud å°ˆç”¨) ---
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="Google News AI æ‘˜è¦", page_icon="ğŸ“°", layout="wide")
st.title("ğŸ“° Google News AI æ‘˜è¦ (è½‰å€ä¿®å¾©ç‰ˆ)")
st.markdown("ä¾†æºï¼š**Google News** | æŠ€è¡“ï¼š**LSA æ‘˜è¦** + **å¼·åŠ›é€£çµè§£ç¢¼**")

# --- 3. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def decode_google_news_url(source_url):
    """
    å¼·åŠ›è§£ç¢¼å‡½å¼ï¼šè§£æ±º Google News RSS çš„è½‰å€å•é¡Œã€‚
    å¦‚æœ requests æ‹¿åˆ°çš„æ˜¯ Google çš„è½‰å€é é¢ï¼Œæ­¤å‡½å¼æœƒå˜—è©¦å¾ HTML ä¸­æŒ–å‡ºçœŸå¯¦é€£çµã€‚
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        }
        
        # 1. ç™¼é€è«‹æ±‚ï¼Œå…è¨±è‡ªå‹•è·³è½‰
        response = requests.get(source_url, headers=headers, timeout=10, allow_redirects=True)
        
        # 2. æª¢æŸ¥æœ€çµ‚ç¶²å€æ˜¯å¦å·²ç¶“é›¢é–‹ Google
        if 'news.google.com' not in response.url and 'google.com' not in response.url:
            return response.url

        # 3. å¦‚æœé‚„åœ¨ Google é é¢ (ä»£è¡¨è¢«æ“‹åœ¨ Consent é æˆ– JS è·³è½‰é )ï¼Œå˜—è©¦è§£æ HTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Google çš„è·³è½‰é é€šå¸¸æœƒæœ‰ä¸€å€‹ä¸»è¦çš„ <a href="..."> é€£çµ
        # æˆ–æ˜¯é€é JS window.location è½‰å€
        
        # å˜—è©¦æ‰¾å°‹é é¢ä¸­ä¸»è¦çš„è½‰å¤–é€£çµ
        # é€™æ˜¯ä¸€å€‹å¸¸è¦‹çš„ Google è½‰å€é é¢ç‰¹å¾µ
        links = soup.find_all('a')
        for link in links:
            href = link.get('href')
            if href and href.startswith('http') and 'google.com' not in href:
                return href
                
        # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå˜—è©¦ç”¨ Regex æœå°‹ JS ä¸­çš„ URL
        match = re.search(r'window\.location\.replace\("(.*?)"\)', response.text)
        if match:
            return match.group(1)

        # å¦‚æœçœŸçš„éƒ½å¤±æ•—ï¼Œå›å‚³åŸå§‹è·³è½‰å¾Œçš„ URL (é›–ç„¶å¯èƒ½é‚„æ˜¯ Google çš„)
        return response.url
        
    except Exception as e:
        # print(f"è§£ç¢¼å¤±æ•—: {e}")
        return source_url

def sumy_summarize(text, sentence_count=3):
    """ä½¿ç”¨ Sumy + Jieba é€²è¡Œä¸­æ–‡èƒå–å¼æ‘˜è¦"""
    try:
        if not text: return "ç„¡å…§å®¹"
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        
        result = ""
        for sentence in summary_sentences:
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
        return result
    except Exception as e:
        return f"æ‘˜è¦éŒ¯èª¤: {e}"

def extract_and_process(google_url):
    """æŠ“å–ä¸¦æ‘˜è¦"""
    try:
        # æ­¥é©Ÿ 1: å¼·åŠ›è§£ç¢¼ (è§£æ±º "Comprehensive..." å•é¡Œçš„é—œéµ)
        real_url = decode_google_news_url(google_url)
        
        # å¦‚æœè§£ç¢¼å¾Œé‚„æ˜¯ google ç¶²å€ï¼Œç›´æ¥è·³éï¼Œå› ç‚ºçˆ¬ä¸åˆ°å…§å®¹
        if "google.com" in real_url:
            return "âš ï¸ ç„¡æ³•è§£æçœŸå¯¦é€£çµ (Google åŠ å¯†è½‰å€)", real_url

        # æ­¥é©Ÿ 2: çˆ¬å–
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
        config.request_timeout = 10
        
        article = Article(real_url, config=config)
        article.download()
        article.parse()
        
        # æ­¥é©Ÿ 3: æª¢æŸ¥ä¸¦æ‘˜è¦
        if len(article.text) < 50:
             if article.meta_description and len(article.meta_description) > 10:
                 return f"ğŸ“Œ (ä¾†æºç°¡ä»‹) {article.meta_description}", real_url
             return "âš ï¸ ç¶²ç«™é˜»æ“‹çˆ¬èŸ² (ç„¡å…§å®¹)", real_url

        summary = sumy_summarize(article.text, sentence_count=3)
        return summary, real_url
        
    except Exception as e:
        return f"âŒ éŒ¯èª¤: {str(e)}", google_url

def search_google_news_rss(keyword, limit=5):
    """Google News RSS"""
    encoded_keyword = requests.utils.quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    feed = feedparser.parse(rss_url)
    
    results = []
    for entry in feed.entries[:limit]:
        results.append({
            "title": entry.title,
            "link": entry.link,
            "published": entry.published
        })
    return results

# --- 4. ä¸»ç¨‹å¼ä»‹é¢ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ã€OpenAI...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æœå°‹')

if submit_button and keyword:
    st.divider()
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    progress_text.text(f"ğŸ” æ­£åœ¨ Google News æœå°‹ã€Œ{keyword}ã€...")
    news_items = search_google_news_rss(keyword, limit=5)
    
    if not news_items:
        st.warning("æ‰¾ä¸åˆ°ç›¸é—œæ–°èã€‚")
        progress_bar.empty()
    else:
        results_data = []
        total = len(news_items)
        
        for i, item in enumerate(news_items):
            progress_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): è§£ç¢¼é€£çµä¸¦æ‘˜è¦... {item['title'][:10]}")
            progress_bar.progress((i + 1) / total)
            
            # å‘¼å« extract_and_process
            summary, real_url = extract_and_process(item['link'])
            
            results_data.append({
                "æ–°èæ¨™é¡Œ": item['title'],
                "AI é‡é»æ‘˜è¦": summary,
                "çœŸå¯¦é€£çµ": real_url
            })
        
        progress_bar.empty()
        progress_text.empty()
        
        st.success("âœ… å®Œæˆï¼")
        
        df = pd.DataFrame(results_data)
        st.dataframe(
            df,
            column_config={
                "çœŸå¯¦é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”— å‰å¾€é–±è®€"),
                "AI é‡é»æ‘˜è¦": st.column_config.TextColumn("AI é‡é»æ‘˜è¦", width="large"),
            },
            hide_index=True,
            use_container_width=True
        )