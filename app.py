import streamlit as st
import pandas as pd
import requests
from newspaper import Article, Config
import jieba
import nltk
from datetime import datetime, timedelta

# --- 1. NLTK è‡ªå‹•ä¿®å¾© ---
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="Massive é‡‘èæ–°èæ‘˜è¦", page_icon="ğŸ¦", layout="wide")
st.title("ğŸ¦ Massive (Polygon) é‡‘èæ–°èæ‘˜è¦")
st.markdown("ä¾†æºï¼š**Massive (Polygon.io)** | æ ¸å¿ƒï¼š**ç¾è‚¡ä»£è™Ÿ (Ticker) æœå°‹**")
st.info("ğŸ’¡ æç¤ºï¼šMassive æ˜¯ç¾è‚¡è³‡æ–™æºï¼Œè«‹è¼¸å…¥ **ç¾è‚¡ä»£è™Ÿ** (ä¾‹å¦‚ï¼š**TSM**, **NVDA**, **AAPL**, **AMD**)")

# --- 3. API Key ---
# Massive (Polygon) API Key
MASSIVE_API_KEY = "vMnBeXpL5XKK4G1nuf2jmXR9B2wXuC15"

# --- 4. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def sumy_summarize(text, sentence_count=3):
    try:
        if not text: return "ç„¡å…§å®¹"
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        result = ""
        for sentence in summary_sentences:
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
        return result
    except Exception as e:
        return f"æ‘˜è¦éŒ¯èª¤: {e}"

def extract_and_process(url):
    try:
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        config.request_timeout = 10
        article = Article(url, config=config)
        article.download()
        article.parse()
        if len(article.text) < 50:
             return "âš ï¸ ç¶²ç«™å…§å®¹éçŸ­ (å»ºè­°é»æ“Šé€£çµé–±è®€)", url
        summary = sumy_summarize(article.text, sentence_count=3)
        return summary, url
    except Exception as e:
        return f"âŒ æŠ“å–éŒ¯èª¤: {str(e)}", url

def search_massive_news(ticker, limit=5):
    """
    ä½¿ç”¨ Massive (Polygon.io) REST API æœå°‹æ–°è
    Docs: https://massive.com/docs/rest/stocks/news
    """
    try:
        # Massive é›–ç„¶æ”¹åï¼Œä½† API ç¶²åŸŸç›®å‰ä»æ²¿ç”¨ Polygon.io
        url = "https://api.polygon.io/v2/reference/news"
        
        params = {
            'ticker': ticker.upper(), # å¼·åˆ¶è½‰å¤§å¯« (ä¾‹å¦‚ tsm -> TSM)
            'limit': limit,
            'apiKey': MASSIVE_API_KEY,
            'sort': 'published_utc',  # æŒ‰æ™‚é–“æ’åº
            'order': 'desc'           # æœ€æ–°çš„åœ¨å‰é¢
        }
        
        response = requests.get(url, params=params)
        data = response.json()
        
        # --- DEBUG å€å¡Š ---
        with st.expander("ğŸ” æŸ¥çœ‹ Massive API åŸå§‹å›å‚³", expanded=False):
            st.json(data)
        # -----------------

        if response.status_code != 200:
            st.error(f"API è«‹æ±‚å¤±æ•—: {data.get('error', 'Unknown Error')}")
            return []

        # Polygon/Massive çš„çµæœåœ¨ 'results' æ¬„ä½ä¸­
        return data.get('results', [])

    except Exception as e:
        st.error(f"é€£ç·šéŒ¯èª¤: {e}")
        return []

# --- 5. ä¸»ç¨‹å¼ä»‹é¢ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        # é è¨­å€¼æ”¹ç‚º TSM (å°ç©é›» ADR)
        keyword = st.text_input("è¼¸å…¥ç¾è‚¡ä»£è™Ÿ (Ticker)", value="TSM", placeholder="ä¾‹å¦‚ï¼šTSM, NVDA, GOOGL")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æœå°‹ Massive')

if submit_button and keyword:
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    progress_text.text(f"ğŸ” æ­£åœ¨æœå°‹ Massive (Polygon) è³‡æ–™åº«: {keyword.upper()}...")
    
    # 1. å‘¼å« API
    articles = search_massive_news(keyword, limit=5)
    
    if not articles:
        st.warning(f"æ‰¾ä¸åˆ°é—œæ–¼ {keyword.upper()} çš„æ–°èã€‚è«‹ç¢ºèªä»£è™Ÿæ˜¯å¦æ­£ç¢º (ä¾‹å¦‚å°ç©é›»è«‹ç”¨ TSM)ã€‚")
        progress_bar.empty()
    else:
        results_data = []
        total = len(articles)
        
        for i, item in enumerate(articles):
            title = item.get('title')
            # Massive çš„æ–°èé€£çµæ¬„ä½é€šå¸¸æ˜¯ 'article_url'
            url = item.get('article_url')
            # Massive æœ¬èº«æœ‰æä¾› descriptionï¼Œå¯ç”¨ä½œå‚™ç”¨æ‘˜è¦
            api_desc = item.get('description', '')
            publisher = item.get('publisher', {}).get('name', 'Unknown')
            
            progress_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): {title[:15]}...")
            progress_bar.progress((i + 1) / total)
            
            # 2. çˆ¬å–èˆ‡æ‘˜è¦
            summary, real_url = extract_and_process(url)
            
            # å¦‚æœçˆ¬èŸ²å¤±æ•—ï¼Œä½¿ç”¨ API è‡ªå¸¶çš„æè¿°
            if summary.startswith("âš ï¸") or summary.startswith("âŒ"):
                summary = f"ğŸ“Œ (å®˜æ–¹æ‘˜è¦) {api_desc}"
            
            results_data.append({
                "æ–°èæ¨™é¡Œ": title,
                "åª’é«”ä¾†æº": publisher,
                "AI é‡é»æ‘˜è¦": summary,
                "ç™¼å¸ƒæ™‚é–“ (UTC)": item.get('published_utc', '')[:10],
                "é€£çµ": real_url
            })
        
        progress_bar.empty()
        progress_text.empty()
        
        st.success(f"âœ… å®Œæˆï¼æ‰¾åˆ° {total} ç¯‡é—œæ–¼ {keyword.upper()} çš„å ±å°ã€‚")
        df = pd.DataFrame(results_data)
        st.dataframe(
            df, 
            column_config={
                "é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”— é–±è®€"),
                "AI é‡é»æ‘˜è¦": st.column_config.TextColumn("AI é‡é»æ‘˜è¦", width="large")
            },
            hide_index=True,
            use_container_width=True
        )