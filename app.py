import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import feedparser
from newspaper import Article
from transformers import pipeline
import time

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="AI æ–°èæ‘˜è¦å°å¹«æ‰‹", page_icon="ğŸ“°", layout="wide")
st.title("ğŸ“° AI Google æ–°èæœå°‹èˆ‡æ‘˜è¦")
st.markdown("è¼¸å…¥é—œéµå­—ï¼ŒAI å°‡ç‚ºæ‚¨æœå°‹å‰ 5 ç¯‡æ–°èä¸¦é€²è¡Œé‡é»æ‘˜è¦ã€‚")
st.markdown("*(æ³¨æ„ï¼šç”±æ–¼åœ¨ CPU ç’°å¢ƒé‹è¡Œæ·±åº¦å­¸ç¿’æ¨¡å‹ï¼Œæ¯ç¯‡æ–‡ç« æ‘˜è¦ç´„éœ€ 10-30 ç§’ï¼Œè«‹è€å¿ƒç­‰å¾…)*")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼å®šç¾© ---

@st.cache_resource
def load_summarizer_model():
    """
    è¼‰å…¥æ‘˜è¦æ¨¡å‹ã€‚ä½¿ç”¨ cache_resource é¿å…æ¯æ¬¡é‡æ–°è¼‰å…¥ã€‚
    é¸æ“‡ 'sshleifer/distilbart-cnn-12-6' æ˜¯å› ç‚ºå®ƒæ¯”æ¨™æº– BART æ¨¡å‹æ›´è¼•é‡ï¼Œé©åˆ CPUã€‚
    """
    with st.spinner('æ­£åœ¨åˆå§‹åŒ– AI æ‘˜è¦æ¨¡å‹ (é¦–æ¬¡åŸ·è¡Œéœ€ä¸‹è¼‰æ¨¡å‹ï¼Œç´„ 300MB)...'):
        # å®šç¾©æ‘˜è¦ pipeline
        summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")
    return summarizer

def search_google_news_rss(keyword, limit=5):
    """
    ä½¿ç”¨ Google News RSS Feed é€²è¡Œæœå°‹ (æ¯”ç›´æ¥çˆ¬ HTML æ›´ç©©å®š)
    """
    encoded_keyword = requests.utils.quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    feed = feedparser.parse(rss_url)
    
    news_list = []
    for entry in feed.entries[:limit]:
        # Google RSS æä¾›çš„é€£çµæ˜¯è·³è½‰é€£çµï¼Œéœ€è¦è§£æå‡ºçœŸå¯¦ URL
        real_url = get_actual_url(entry.link)
        if real_url:
            news_list.append({
                "title": entry.title,
                "link": real_url,
                "published": entry.published
            })
    return news_list

def get_actual_url(google_url):
    """
    å¾ Google News çš„è·³è½‰é€£çµä¸­ç²å–çœŸå¯¦çš„ç¶²ç«™é€£çµã€‚
    """
    try:
        # è¨­å®š User-Agent æ¨¡æ“¬ç€è¦½å™¨è¡Œç‚ºï¼Œé¿å…è¢«æ“‹
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # ç™¼é€è«‹æ±‚ä¸¦ç²å–æœ€çµ‚è·³è½‰å¾Œçš„ URL
        response = requests.head(google_url, allow_redirects=True, headers=headers, timeout=5)
        return response.url
    except Exception as e:
        # print(f"è§£æ URL å¤±æ•—: {e}")
        return None

def extract_and_summarize(url, summarizer_pipeline):
    """
    æŠ“å–æ–°èå…§æ–‡ä¸¦å‘¼å« AI æ¨¡å‹é€²è¡Œæ‘˜è¦
    """
    try:
        # 1. ä½¿ç”¨ newspaper3k æŠ“å–æ–‡ç« å…§å®¹
        article = Article(url)
        article.download()
        article.parse()
        
        text_content = article.text
        
        if len(text_content) < 200:
             return "æ–‡ç« å…§å®¹å¤ªçŸ­ï¼Œç„¡æ³•é€²è¡Œæœ‰æ•ˆæ‘˜è¦ã€‚"

        # 2. ä½¿ç”¨ Transformers é€²è¡Œæ‘˜è¦
        # max_length: æ‘˜è¦æœ€å¤§é•·åº¦, min_length: æ‘˜è¦æœ€å°é•·åº¦
        # ç‚ºäº†é€Ÿåº¦ï¼Œæˆ‘å€‘é™åˆ¶è¼¸å…¥æ–‡æœ¬çš„é•·åº¦ (truncation=True)
        summary_result = summarizer_pipeline(text_content, max_length=130, min_length=50, do_sample=False, truncation=True)
        
        return summary_result[0]['summary_text']
        
    except Exception as e:
        return f"æŠ“å–æˆ–æ‘˜è¦å¤±æ•—: {str(e)}"

# --- 3. ä¸»ç¨‹å¼é‚è¼¯ ---

# é å…ˆè¼‰å…¥æ¨¡å‹
try:
    summarizer = load_summarizer_model()
    st.success("AI æ¨¡å‹æº–å‚™å°±ç·’ï¼")
except Exception as e:
    st.error(f"æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥è¨˜æ†¶é«”æˆ–ç¶²è·¯ç‹€æ…‹: {e}")
    st.stop()


# ä½¿ç”¨è€…è¼¸å…¥ä»‹é¢
with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è«‹è¼¸å…¥æ–°èé—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ã€äººå·¥æ™ºæ…§...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸ” é–‹å§‹æœå°‹èˆ‡æ‘˜è¦')

if submit_button and keyword:
    st.divider()
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    results_data = []

    # æ­¥é©Ÿ 1: æœå°‹æ–°è
    status_text.info(f"æ­£åœ¨æœå°‹ã€Œ{keyword}ã€çš„ç›¸é—œæ–°è...")
    news_items = search_google_news_rss(keyword)
    
    if not news_items:
        st.warning("æ‰¾ä¸åˆ°ç›¸é—œæ–°èï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚")
    else:
        total_items = len(news_items)
        
        # æ­¥é©Ÿ 2 & 3: é€ä¸€æŠ“å–å…§å®¹ä¸¦æ‘˜è¦
        for i, item in enumerate(news_items):
            status_text.info(f"æ­£åœ¨è™•ç†ç¬¬ {i+1}/{total_items} ç¯‡æ–°èï¼š{item['title']}...")
            progress_bar.progress((i) / total_items)
            
            # åŸ·è¡Œæ‘˜è¦ (é€™ä¸€æ­¥æœ€èŠ±æ™‚é–“)
            summary = extract_and_summarize(item['link'], summarizer)
            
            results_data.append({
                "æ–°èæ¨™é¡Œ": item['title'],
                "AI æ‘˜è¦": summary,
                "åŸå§‹é€£çµ": item['link']
            })
        
        progress_bar.progress(100)
        status_text.success("âœ… æ‰€æœ‰æ–°èè™•ç†å®Œæˆï¼")
        time.sleep(1)
        status_text.empty()
        progress_bar.empty()

        # æ­¥é©Ÿ 4: ä»¥è¡¨æ ¼å‘ˆç¾çµæœ
        st.subheader(f"ğŸ“Š ã€Œ{keyword}ã€çš„æ–°èæ‘˜è¦çµæœ")
        
        # å°‡è³‡æ–™è½‰æ›ç‚º Pandas DataFrame
        df = pd.DataFrame(results_data)
        
        # ä½¿ç”¨ Streamlit çš„ dataframe å±•ç¤ºï¼Œä¸¦è¨­å®šé€£çµæ¬„ä½é¡¯ç¤ºç‚ºå¯é»æ“Šçš„ URL
        st.dataframe(
            df,
            column_config={
                "åŸå§‹é€£çµ": st.column_config.LinkColumn("åŸå§‹é€£çµ", display_text="é»æ“Šé–±è®€åŸæ–‡")
            },
            hide_index=True,
            use_container_width=True
        )