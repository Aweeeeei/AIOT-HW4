import streamlit as st
import pandas as pd
import requests
from newspaper import Article, Config
import jieba
import nltk
from datetime import datetime

# --- 1. NLTK è‡ªå‹•ä¿®å¾© ---
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="GNews AI æ–°èåŠ©æ‰‹", page_icon="ğŸ“¡", layout="wide")
st.title("ğŸ“¡ GNews AI æ–°èæ‘˜è¦åŠ©æ‰‹")
st.markdown("ä¾†æºï¼š**GNews API** | æŠ€è¡“ï¼š**RESTful API** + **LSA æ¼”ç®—æ³•**")

# --- 3. è¨­å®š API Key ---
# å»ºè­°ï¼šå¯¦éš›éƒ¨ç½²æ™‚ï¼Œæœ€å¥½å°‡ API Key æ”¾åœ¨ st.secretsï¼Œä½†ä½œæ¥­ç¹³äº¤ç›´æ¥å¯«åœ¨è®Šæ•¸ä¹Ÿå¯ä»¥
GNEWS_API_KEY = "b8bba61d5cec4532cc9b3630311eed30"

# --- 4. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def sumy_summarize(text, sentence_count=3):
    """ä½¿ç”¨ Sumy + Jieba é€²è¡Œä¸­æ–‡èƒå–å¼æ‘˜è¦"""
    try:
        if not text: return "ç„¡å…§å®¹"
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        
        result = ""
        for sentence in summary_sentences:
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
        return result
    except Exception as e:
        return f"æ‘˜è¦éŒ¯èª¤: {e}"

def extract_and_process(url):
    """
    æŠ“å–ä¸¦æ‘˜è¦
    GNews çµ¦çš„æ˜¯ç›´é€£ç¶²å€ï¼Œæˆ‘å€‘ç›´æ¥ç”¨ newspaper3k æŠ“å–å³å¯ã€‚
    """
    try:
        # è¨­å®šå½è£ç€è¦½å™¨ (é›–ç„¶ API çµ¦äº†é€£çµï¼Œä½†ç›®æ¨™æ–°èç¶²ç«™å¯èƒ½é‚„æ˜¯æœƒæ“‹çˆ¬èŸ²)
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        config.request_timeout = 10
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        
        # æª¢æŸ¥å…§å®¹é•·åº¦
        if len(article.text) < 50:
             return "âš ï¸ ç¶²ç«™å…§å®¹éçŸ­æˆ–é˜»æ“‹çˆ¬èŸ² (å»ºè­°é»æ“Šé€£çµé–±è®€)", url

        # åŸ·è¡Œæ‘˜è¦
        summary = sumy_summarize(article.text, sentence_count=3)
        return summary, url
        
    except Exception as e:
        return f"âŒ æŠ“å–éŒ¯èª¤: {str(e)}", url

def search_gnews(keyword, limit=5):
    """
    ä½¿ç”¨ GNews API é€²è¡Œæœå°‹
    æ–‡ä»¶ï¼šhttps://gnews.io/docs/v4
    """
    try:
        url = "https://gnews.io/api/v4/search"
        params = {
            'q': keyword,
            'token': GNEWS_API_KEY,
            'lang': 'zh',       # èªè¨€ï¼šä¸­æ–‡
            'country': 'tw',    # åœ‹å®¶ï¼šå°ç£
            'max': limit,       # æ•¸é‡é™åˆ¶
            'sortby': 'publishedAt' # æŒ‰æ™‚é–“æ’åº
        }
        
        response = requests.get(url, params=params)
        data = response.json()
        
        if response.status_code != 200:
            st.error(f"API è«‹æ±‚å¤±æ•—: {data.get('errors', 'Unknown error')}")
            return []

        articles = data.get('articles', [])
        return articles

    except Exception as e:
        st.error(f"é€£ç·šéŒ¯èª¤: {e}")
        return []

# --- 5. ä¸»ç¨‹å¼ä»‹é¢ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ã€AI...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æœå°‹ GNews')

if submit_button and keyword:
    st.divider()
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    progress_text.text(f"ğŸ” æ­£åœ¨å‘¼å« GNews API æœå°‹ã€Œ{keyword}ã€...")
    
    # 1. å‘¼å« API
    articles = search_gnews(keyword, limit=5)
    
    if not articles:
        st.warning("æ‰¾ä¸åˆ°ç›¸é—œæ–°èï¼Œæˆ– API é…é¡å·²ç”¨ç›¡ã€‚")
        progress_bar.empty()
    else:
        results_data = []
        total = len(articles)
        
        for i, item in enumerate(articles):
            title = item.get('title')
            url = item.get('url')
            # GNews API æœ¬èº«æœ‰æä¾› descriptionï¼Œå¦‚æœçˆ¬èŸ²å¤±æ•—å¯ä»¥ç”¨é€™å€‹ç•¶å‚™æ¡ˆ
            api_description = item.get('description', '')
            
            progress_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): {title[:15]}...")
            progress_bar.progress((i + 1) / total)
            
            # 2. çˆ¬å–å…§æ–‡ä¸¦ LSA æ‘˜è¦
            summary, real_url = extract_and_process(url)
            
            # å¦‚æœçˆ¬èŸ²å¤±æ•— (summary é–‹é ­æ˜¯ âš ï¸ æˆ– âŒ)ï¼Œå›é€€ä½¿ç”¨ API æä¾›çš„ç°¡çŸ­æè¿°
            if summary.startswith("âš ï¸") or summary.startswith("âŒ"):
                summary = f"ğŸ“Œ (API åŸæ–‡æ‘˜è¦) {api_description}"
            
            results_data.append({
                "æ–°èæ¨™é¡Œ": title,
                "AI é‡é»æ‘˜è¦": summary,
                "ç™¼å¸ƒæ™‚é–“": item.get('publishedAt', '')[:10], # åªå–æ—¥æœŸ
                "é€£çµ": real_url
            })
        
        progress_bar.empty()
        progress_text.empty()
        
        st.success(f"âœ… å®Œæˆï¼å…±æœå°‹åˆ° {total} ç¯‡æ–°èã€‚")
        
        df = pd.DataFrame(results_data)
        st.dataframe(
            df,
            column_config={
                "é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”— é–±è®€åŸæ–‡"),
                "AI é‡é»æ‘˜è¦": st.column_config.TextColumn("AI é‡é»æ‘˜è¦", width="large"),
            },
            hide_index=True,
            use_container_width=True
        )