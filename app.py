import streamlit as st
import pandas as pd
import requests
import feedparser
from newspaper import Article, Config
import time
import jieba
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="æ¥µé€Ÿæ–°èæ‘˜è¦åŠ©æ‰‹", page_icon="âš¡", layout="wide")
st.title("âš¡ æ¥µé€Ÿç‰ˆ Google æ–°èæ‘˜è¦ (çˆ¬èŸ²å¼·åŒ–ç‰ˆ)")
st.markdown("ä½¿ç”¨ **LSA æ¼”ç®—æ³•** + **å½è£ç€è¦½å™¨çˆ¬èŸ²**ï¼Œè§£æ±ºæŠ“å–å¤±æ•—å•é¡Œã€‚")

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def get_actual_url(google_url):
    """
    å˜—è©¦è§£æ Google News çš„è·³è½‰é€£çµï¼Œç²å–çœŸå¯¦ç¶²å€ã€‚
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        }
        # ä½¿ç”¨ requests.get è€Œé headï¼Œä¸¦é–‹å•Ÿ allow_redirects
        # Google æœ‰æ™‚æœƒç”¨ JS è·³è½‰ï¼Œé€™è£¡å˜—è©¦ç²å–æœ€çµ‚éŸ¿æ‡‰çš„ URL
        response = requests.get(google_url, headers=headers, timeout=10, allow_redirects=True)
        
        # å¦‚æœç¶²å€é‚„æ˜¯ google çš„ï¼Œä»£è¡¨è·³è½‰æ²’æˆåŠŸ (å¯èƒ½æ˜¯ JS è½‰å€)ï¼Œé€™æ™‚åªèƒ½å›å‚³åŸç¶²å€è©¦è©¦é‹æ°£
        if "news.google.com" in response.url:
            return google_url
        return response.url
    except Exception as e:
        print(f"URL è§£æéŒ¯èª¤: {e}")
        return google_url # è§£æå¤±æ•—å°±å›å‚³åŸç¶²å€

def sumy_summarize(text, sentence_count=3):
    """ä½¿ç”¨ Sumy + Jieba é€²è¡Œä¸­æ–‡èƒå–å¼æ‘˜è¦"""
    try:
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        
        result = ""
        for sentence in summary_sentences:
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
        return result
    except Exception as e:
        return f"æ‘˜è¦éŒ¯èª¤: {e}"

def extract_and_process(url):
    """
    æŠ“å–ä¸¦æ‘˜è¦ (åŠ å…¥ Config é˜²æ­¢è¢«æ“‹)
    """
    try:
        # --- é—œéµä¿®æ­£ï¼šè¨­å®š Config å½è£æˆç€è¦½å™¨ ---
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'
        config.request_timeout = 10  # è¨­å®šè¶…æ™‚
        config.fetch_images = False  # ä¸æŠ“åœ–ç‰‡ï¼ŒåŠ é€Ÿ
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        
        # debug ç”¨ï¼šå¦‚æœæŠ“ä¸åˆ°å­—ï¼Œé¡¯ç¤ºé•·åº¦
        text_len = len(article.text)
        
        if text_len < 50:
             # å˜—è©¦å¦ä¸€ç¨®å®¹éŒ¯ï¼šå¦‚æœæ­£æ–‡æŠ“ä¸åˆ°ï¼Œè©¦è©¦çœ‹æŠ“ meta description
             if article.meta_description and len(article.meta_description) > 20:
                 return f"(ä½¿ç”¨ Meta æè¿°) {article.meta_description}"
             
             return f"âš ï¸ ç„¡æ³•æŠ“å–å…§å®¹ (é•·åº¦åƒ… {text_len} å­—) - å¯èƒ½æ˜¯ç¶²ç«™é˜»æ“‹æˆ–éœ€ç™»å…¥"

        # ä½¿ç”¨ Sumy é€²è¡Œæ‘˜è¦
        summary = sumy_summarize(article.text, sentence_count=3)
        if not summary:
            return "æ‘˜è¦ç”¢ç”Ÿå¤±æ•— (å…§å®¹å¯èƒ½éæ–¼ç ´ç¢)"
            
        return summary
        
    except Exception as e:
        return f"âŒ è™•ç†éŒ¯èª¤: {str(e)}"

def search_google_news_rss(keyword, limit=5):
    encoded_keyword = requests.utils.quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={encoded_keyword}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    feed = feedparser.parse(rss_url)
    
    news_list = []
    for entry in feed.entries[:limit]:
        # å…ˆçµ¦ Google é€£çµï¼Œå¾ŒçºŒè™•ç†æ™‚å†è§£æ
        news_list.append({
            "title": entry.title,
            "link": entry.link, 
            "published": entry.published
        })
    return news_list

# --- 3. ä¸»ç¨‹å¼é‚è¼¯ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»ã€è¼é”...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æœå°‹ä¸¦æ‘˜è¦')

if submit_button and keyword:
    st.divider()
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    
    progress_text.text("æ­£åœ¨æœå°‹æ–°èä¾†æº...")
    news_items = search_google_news_rss(keyword)
    
    results_data = []
    total = len(news_items)
    
    for i, item in enumerate(news_items):
        progress_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): {item['title']} ...")
        progress_bar.progress((i + 1) / total)
        
        # 1. å˜—è©¦è§£æçœŸå¯¦ URL
        real_url = get_actual_url(item['link'])
        
        # 2. æŠ“å–èˆ‡æ‘˜è¦
        summary = extract_and_process(real_url)
        
        results_data.append({
            "æ–°èæ¨™é¡Œ": item['title'],
            "é‡é»æ‘˜è¦": summary,
            "åŸå§‹é€£çµ": real_url
        })
    
    progress_bar.empty()
    progress_text.empty()
            
    if results_data:
        st.success(f"å·²å®Œæˆï¼")
        df = pd.DataFrame(results_data)
        st.dataframe(
            df,
            column_config={
                "åŸå§‹é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”—"),
                "é‡é»æ‘˜è¦": st.column_config.TextColumn("é‡é»æ‘˜è¦", width="large")
            },
            hide_index=True,
            use_container_width=True
        )
    else:
        st.warning("æ‰¾ä¸åˆ°ç›¸é—œæ–°èã€‚")