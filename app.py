import streamlit as st
import pandas as pd
import requests
from newspaper import Article, Config
import jieba
import nltk
from datetime import datetime, timedelta

# --- 1. NLTK è‡ªå‹•ä¿®å¾© ---
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab', quiet=True)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)

from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer

# --- 2. é é¢è¨­å®š ---
st.set_page_config(page_title="GNews AI æ–°èåŠ©æ‰‹", page_icon="ğŸ“¡", layout="wide")
st.title("ğŸ“¡ GNews AI æ–°èæ‘˜è¦åŠ©æ‰‹ (å…è²»ç‰ˆå„ªåŒ–)")
st.markdown("ä¾†æºï¼š**GNews API** | å„ªåŒ–ï¼š**è‡ªå‹•é–å®šæœ€è¿‘ 30 å¤©æ–°è**")

# --- 3. API Key ---
GNEWS_API_KEY = "b8bba61d5cec4532cc9b3630311eed30"

# --- 4. æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

def sumy_summarize(text, sentence_count=3):
    try:
        if not text: return "ç„¡å…§å®¹"
        seg_list = jieba.cut(text)
        text_segmented = " ".join(seg_list)
        parser = PlaintextParser.from_string(text_segmented, Tokenizer("english")) 
        summarizer = LsaSummarizer() 
        summary_sentences = summarizer(parser.document, sentence_count)
        result = ""
        for sentence in summary_sentences:
            raw_sent = str(sentence).replace(" ", "")
            result += raw_sent + "ã€‚"
        return result
    except Exception as e:
        return f"æ‘˜è¦éŒ¯èª¤: {e}"

def extract_and_process(url):
    try:
        config = Config()
        config.browser_user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        config.request_timeout = 10
        article = Article(url, config=config)
        article.download()
        article.parse()
        if len(article.text) < 50:
             return "âš ï¸ ç¶²ç«™å…§å®¹éçŸ­ (å»ºè­°é»æ“Šé€£çµé–±è®€)", url
        summary = sumy_summarize(article.text, sentence_count=3)
        return summary, url
    except Exception as e:
        return f"âŒ æŠ“å–éŒ¯èª¤: {str(e)}", url

def search_gnews(keyword, limit=5):
    """
    ä½¿ç”¨ GNews API é€²è¡Œæœå°‹ (é‡å°å…è²»ç‰ˆé™åˆ¶é€²è¡Œå„ªåŒ–)
    """
    try:
        url = "https://gnews.io/api/v4/search"
        
        # --- é—œéµä¿®æ­£ï¼šè¨ˆç®— 28 å¤©å‰çš„æ™‚é–“å­—ä¸² ---
        # å…è²»ç‰ˆåªèƒ½çœ‹éå» 30 å¤©ï¼Œæˆ‘å€‘è¨­ 28 å¤©æ¯”è¼ƒä¿éšª
        past_date = datetime.utcnow() - timedelta(days=28)
        from_date_str = past_date.strftime('%Y-%m-%dT%H:%M:%SZ')
        # -------------------------------------

        params = {
            'q': keyword,
            'token': GNEWS_API_KEY,
            'lang': 'zh',
            'country': 'tw',
            'max': limit,
            'sortby': 'publishedAt',
            'from': from_date_str # å¼·åˆ¶åªæ‰¾é€™æ®µæ™‚é–“å…§çš„æ–°è
        }
        
        response = requests.get(url, params=params)
        data = response.json()
        
        # é™¤éŒ¯è³‡è¨Šï¼šå¦‚æœé‚„æ˜¯ç©ºçš„ï¼Œå¯ä»¥åœ¨é€™è£¡çœ‹åˆ°åŸå› 
        if 'articles' not in data or len(data['articles']) == 0:
            # å¦‚æœé‚„æ˜¯ç©ºçš„ï¼Œå˜—è©¦æ”¾å¯¬æ¢ä»¶ (ç§»é™¤ country é™åˆ¶) å†æœä¸€æ¬¡
            if 'country' in params:
                del params['country']
                response = requests.get(url, params=params)
                data = response.json()

        if response.status_code != 200:
            st.error(f"API ç‹€æ…‹ç¢¼éŒ¯èª¤: {response.status_code}")
            return []

        articles = data.get('articles', [])
        return articles

    except Exception as e:
        st.error(f"é€£ç·šéŒ¯èª¤: {e}")
        return []

# --- 5. ä¸»ç¨‹å¼ä»‹é¢ ---

with st.form(key='search_form'):
    col1, col2 = st.columns([3, 1])
    with col1:
        keyword = st.text_input("è¼¸å…¥é—œéµå­—", placeholder="ä¾‹å¦‚ï¼šå°ç©é›»...")
    with col2:
        submit_button = st.form_submit_button(label='ğŸš€ æœå°‹')

if submit_button and keyword:
    
    progress_text = st.empty()
    progress_bar = st.progress(0)
    progress_text.text(f"ğŸ” æ­£åœ¨æœå°‹æœ€è¿‘ä¸€å€‹æœˆçš„ GNews...")
    
    # 1. å‘¼å« API
    articles = search_gnews(keyword, limit=5)
    
    if not articles:
        st.warning("âš ï¸ æœå°‹çµæœç‚ºç©ºã€‚å¯èƒ½æ˜¯è©²é—œéµå­—åœ¨éå» 30 å¤©å…§ç„¡æ–°èï¼Œæˆ–å‰›å¥½è¢« API é™åˆ¶éæ¿¾ã€‚")
        progress_bar.empty()
    else:
        results_data = []
        total = len(articles)
        
        for i, item in enumerate(articles):
            title = item.get('title')
            url = item.get('url')
            api_desc = item.get('description', '')
            
            progress_text.text(f"æ­£åœ¨è™•ç† ({i+1}/{total}): {title[:15]}...")
            progress_bar.progress((i + 1) / total)
            
            # 2. çˆ¬å–èˆ‡æ‘˜è¦
            summary, real_url = extract_and_process(url)
            
            # å¦‚æœçˆ¬èŸ²å¤±æ•—ï¼Œä½¿ç”¨ API è‡ªå¸¶çš„æè¿°
            if summary.startswith("âš ï¸") or summary.startswith("âŒ"):
                summary = f"ğŸ“Œ (API æ‘˜è¦) {api_desc}"
            
            results_data.append({
                "æ¨™é¡Œ": title,
                "AI æ‘˜è¦": summary,
                "æ™‚é–“": item.get('publishedAt', '')[:10],
                "é€£çµ": real_url
            })
        
        progress_bar.empty()
        progress_text.empty()
        
        st.success(f"âœ… å®Œæˆï¼å…±æœå°‹åˆ° {total} ç¯‡æ–°èã€‚")
        df = pd.DataFrame(results_data)
        st.dataframe(
            df, 
            column_config={"é€£çµ": st.column_config.LinkColumn("é€£çµ", display_text="ğŸ”— é–±è®€")},
            hide_index=True,
            use_container_width=True
        )